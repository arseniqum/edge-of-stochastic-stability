{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# W&B Checkpoint Loader and Hessian Tools\n",
        "Utility notebook to restore a training run at a chosen step, reload the dataset, compute Hessian eigenvalues, and run additional optimizer steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Usage**\n",
        "- Set `RUN_ID` (or `RUN_PATH`) and `TARGET_STEP` below.\n",
        "- Ensure `DATASETS` & `WANDB_DIR` environment variables point to your dataset root and W&B workspace.\n",
        "- Execute cells sequentially to inspect eigenvalues or continue optimization from the restored checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
        "\n",
        "\n",
        "sys.path.append('..')\n",
        "\n",
        "from utils.data import prepare_dataset\n",
        "from utils.nets import prepare_net_dataset_specific, initialize_net, prepare_optimizer\n",
        "from utils.wandb_utils import find_closest_checkpoint_wandb, load_checkpoint_wandb, get_checkpoint_dir_for_run\n",
        "from utils.measure import compute_eigenvalues_hvp, compute_eigenvalues, create_eigenvector_cache\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WANDB_BASE_DIR: /scratch/gpfs/andreyev/eoss/results\n",
            "DATASETS_ROOT: /scratch/gpfs/andreyev/datasets\n"
          ]
        }
      ],
      "source": [
        "# --- User configuration (edit as needed) ---\n",
        "\n",
        "WANDB_BASE_DIR = Path(os.environ.get('WANDB_DIR', '.'))\n",
        "# DATASETS_ROOT = Path(os.environ.get('DATASETS', '/scratch/gpfs/andreyev/datasets'))\n",
        "DATASETS_ROOT = Path('/scratch/gpfs/andreyev/datasets')\n",
        "\n",
        "RUN_ID = 'zc9aoqya'  # e.g. 'abcd1234'\n",
        "RUN_PATH = None  # Optionally provide Path('/absolute/path/to/wandb/run-XYZ-abcd1234')\n",
        "TARGET_STEP = 10000  # desired training step to restore (closest checkpoint â‰¤ this is used)\n",
        "\n",
        "CHECKPOINT_DIR = None  # Optionally override checkpoint directory Path('.../wandb_checkpoints/abcd1234')\n",
        "BATCH_SIZE_OVERRIDE = None  # Set to int to override logged batch size\n",
        "LEARNING_RATE_OVERRIDE = None  # Set to float to override logged learning rate\n",
        "\n",
        "NUM_EIGENVALUES = 1  # top-k Hessian eigenvalues to compute\n",
        "USE_HVP_FOR_EIGS = True  # True -> use HVP-based routine, False -> autograd LOBPCG\n",
        "\n",
        "print('WANDB_BASE_DIR:', WANDB_BASE_DIR)\n",
        "print('DATASETS_ROOT:', DATASETS_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "def extract_run_id(run_dir: Path) -> Optional[str]:\n",
        "    meta_path = run_dir / \"files\" / \"wandb-metadata.json\"\n",
        "    if meta_path.exists():\n",
        "        try:\n",
        "            meta = json.loads(meta_path.read_text())\n",
        "            rid = meta.get(\"id\")\n",
        "            if rid:\n",
        "                return rid\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "\n",
        "def locate_run_directory(run_id: Optional[str] = None,\n",
        "                         run_path: Optional[Path] = None,\n",
        "                         base_dir: Path = WANDB_BASE_DIR) -> Tuple[Path, str]:\n",
        "    if run_path is not None:\n",
        "        run_dir = Path(run_path).expanduser().resolve()\n",
        "        if not run_dir.exists():\n",
        "            raise FileNotFoundError(f\"Provided RUN_PATH does not exist: {run_dir}\")\n",
        "        resolved_id = run_id or extract_run_id(run_dir)\n",
        "        if resolved_id is None:\n",
        "            raise ValueError(\"Could not infer run id from RUN_PATH; please set RUN_ID explicitly.\")\n",
        "        return run_dir, resolved_id\n",
        "\n",
        "    if run_id is None:\n",
        "        raise ValueError(\"Set RUN_ID or RUN_PATH to locate the Weights & Biases run.\")\n",
        "\n",
        "    run_id = run_id.strip()\n",
        "    if not base_dir.exists():\n",
        "        raise FileNotFoundError(f\"WANDB_BASE_DIR does not exist: {base_dir}\")\n",
        "\n",
        "    base_dir = base_dir / 'wandb'\n",
        "    candidates = sorted(base_dir.glob(\"offline-run-*\"))\n",
        "    # checkpoint_base_dir = wandb_dir / \"wandb_checkpoints\"\n",
        "\n",
        "    for candidate in candidates:\n",
        "        candidate_id = extract_run_id(candidate)\n",
        "        if candidate_id == run_id:\n",
        "            return candidate, run_id\n",
        "        if candidate.name.endswith(run_id):\n",
        "            return candidate, run_id\n",
        "\n",
        "    raise FileNotFoundError(f\"Could not find run directory for id='{run_id}' under {base_dir}.\")\n",
        "\n",
        "\n",
        "def _coerce_config_value(value: Any) -> Any:\n",
        "    if isinstance(value, dict) and \"value\" in value:\n",
        "        value = value[\"value\"]\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        text = value.strip().strip('\"').strip(\"'\")\n",
        "        lowered = text.lower()\n",
        "        if lowered in {\"none\", \"null\"}:\n",
        "            return None\n",
        "        if lowered == \"true\":\n",
        "            return True\n",
        "        if lowered == \"false\":\n",
        "            return False\n",
        "        try:\n",
        "            return json.loads(text)\n",
        "        except json.JSONDecodeError:\n",
        "            return text\n",
        "    return value\n",
        "\n",
        "\n",
        "def load_wandb_config(run_dir: Path) -> Dict[str, Any]:\n",
        "    files_dir = run_dir / \"files\"\n",
        "    json_path = files_dir / \"config.json\"\n",
        "    if json_path.exists():\n",
        "        raw = json.loads(json_path.read_text())\n",
        "        return {k: _coerce_config_value(v) for k, v in raw.items() if not k.startswith(\"_\")}\n",
        "\n",
        "    yaml_path = files_dir / \"config.yaml\"\n",
        "    if yaml_path.exists():\n",
        "        text = yaml_path.read_text()\n",
        "        try:\n",
        "            import yaml  # type: ignore\n",
        "            raw = yaml.safe_load(text)\n",
        "            return {\n",
        "                k: _coerce_config_value(v.get(\"value\") if isinstance(v, dict) else v)\n",
        "                for k, v in raw.items()\n",
        "                if not k.startswith(\"_\")\n",
        "            }\n",
        "        except ImportError:\n",
        "            config: Dict[str, Any] = {}\n",
        "            current_key: Optional[str] = None\n",
        "            for line in text.splitlines():\n",
        "                if not line.strip():\n",
        "                    continue\n",
        "                if not line.startswith(\"  \"):\n",
        "                    current_key = line.split(\":\", 1)[0].strip()\n",
        "                elif \"value:\" in line and current_key:\n",
        "                    value_text = line.split(\"value:\", 1)[1].strip()\n",
        "                    config[current_key] = _coerce_config_value(value_text)\n",
        "            return config\n",
        "\n",
        "    raise FileNotFoundError(f\"No config file found in {files_dir}\")\n",
        "\n",
        "\n",
        "def ensure_list(value: Any):\n",
        "    if value is None:\n",
        "        return value\n",
        "    if isinstance(value, (list, tuple)):\n",
        "        return list(value)\n",
        "    if isinstance(value, str):\n",
        "        try:\n",
        "            parsed = json.loads(value)\n",
        "            if isinstance(parsed, list):\n",
        "                return parsed\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "        parts = [p.strip() for p in value.split(',') if p.strip()]\n",
        "        if parts:\n",
        "            result = []\n",
        "            for p in parts:\n",
        "                if p.isdigit():\n",
        "                    result.append(int(p))\n",
        "                else:\n",
        "                    result.append(p)\n",
        "            return result\n",
        "        return []\n",
        "    return [value]\n",
        "\n",
        "\n",
        "def build_loss_fn(name: str) -> nn.Module:\n",
        "    if not name:\n",
        "        name = \"mse\"\n",
        "    name = name.lower()\n",
        "    if name in {\"mse\", \"l2\", \"squared\"}:\n",
        "        return nn.MSELoss()\n",
        "    if name in {\"ce\", \"cross_entropy\", \"crossentropy\"}:\n",
        "        return nn.CrossEntropyLoss()\n",
        "    raise ValueError(f\"Unsupported loss '{name}'\")\n",
        "\n",
        "\n",
        "def make_data_cycle(X: torch.Tensor, Y: torch.Tensor, batch_size: int, shuffle: bool = True):\n",
        "    dataset = TensorDataset(X, Y)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
        "    def iterator():\n",
        "        while True:\n",
        "            for batch in loader:\n",
        "                yield batch\n",
        "    return loader, iterator()\n",
        "\n",
        "\n",
        "def compute_top_hessian_for_batch(model: nn.Module,\n",
        "                                  inputs: torch.Tensor,\n",
        "                                  targets: torch.Tensor,\n",
        "                                  loss_fn: nn.Module,\n",
        "                                  k: int = 1,\n",
        "                                  use_hvp: bool = True,\n",
        "                                  eigenvector_cache=None,\n",
        "                                  **kwargs):\n",
        "    model.eval()\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    if use_hvp:\n",
        "        eigenvalues, eigenvectors = compute_eigenvalues_hvp(\n",
        "            model,\n",
        "            inputs,\n",
        "            targets,\n",
        "            loss_fn,\n",
        "            k=k,\n",
        "            eigenvector_cache=eigenvector_cache,\n",
        "            return_eigenvectors=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "    else:\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        eigen_result = compute_eigenvalues(\n",
        "            loss,\n",
        "            model,\n",
        "            k=k,\n",
        "            eigenvector_cache=eigenvector_cache,\n",
        "            return_eigenvectors=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "        if k == 1:\n",
        "            eigen_value, eigen_vector = eigen_result\n",
        "            eigenvalues = torch.as_tensor([float(eigen_value)])\n",
        "            eigenvectors = eigen_vector.unsqueeze(1)\n",
        "        else:\n",
        "            eigenvalues, eigenvectors = eigen_result\n",
        "    return eigenvalues.detach().cpu(), eigenvectors\n",
        "\n",
        "\n",
        "def run_optimizer_steps(model: nn.Module,\n",
        "                        optimizer: torch.optim.Optimizer,\n",
        "                        data_iter,\n",
        "                        loss_fn: nn.Module,\n",
        "                        num_steps: int,\n",
        "                        log_every: int = 1):\n",
        "    model.train()\n",
        "    history = []\n",
        "    for step in range(1, num_steps + 1):\n",
        "        inputs, targets = next(data_iter)\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_value = float(loss.detach().cpu())\n",
        "        history.append(loss_value)\n",
        "        if log_every and step % log_every == 0:\n",
        "            print(f\"Step {step:04d} | loss: {loss_value:.6f}\")\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using run directory: /scratch/gpfs/andreyev/eoss/results/wandb/offline-run-20250822_090227-zc9aoqya\n",
            "Loaded 50 config entries. Keys: ['adam', 'batch', 'batch_lambdamax', 'batch_sharpness', 'batch_sharpness_mc', 'batch_sharpness_static', 'batch_sharpness_step', 'checkpoint_every', 'classes', 'cont_epoch', 'cont_folder', 'cont_last', 'cont_run_id', 'cont_step', 'dataset', 'dataset_seed', 'disable_cache_eigenvectors', 'epochs', 'final', 'fisher', 'gd_noise', 'gni', 'gradient_norm', 'init_scale', 'init_seed', 'init_sharp', 'lambdamax', 'loss', 'lr', 'model', 'momentum', 'no_init', 'noise_mag', 'num_data', 'num_eigenvalues', 'param_distance', 'param_file', 'quad_switch_step', 'results_rarely', 'sde', 'sde_eta', 'sde_h', 'sde_seed', 'sharp_every', 'steps', 'stop_loss', 'use_gauss_newton', 'use_power_iteration', 'wandb_tag', 'wandb_version']\n"
          ]
        }
      ],
      "source": [
        "run_dir, resolved_run_id = locate_run_directory(run_id=RUN_ID or None, run_path=RUN_PATH)\n",
        "print(f'Using run directory: {run_dir}')\n",
        "config = load_wandb_config(run_dir)\n",
        "print(f'Loaded {len(config)} config entries. Keys: {sorted(config.keys())}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training tensors: (8192, 3, 32, 32) | Targets: (8192, 10)\n",
            "Test tensors: (10000, 3, 32, 32) | Targets: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "dataset_name = config.get('dataset', 'cifar10')\n",
        "num_data = int(config.get('num_data', 1024))\n",
        "classes = ensure_list(config.get('classes', [1, 9])) or [1, 9]\n",
        "dataset_seed = int(config.get('dataset_seed', 888))\n",
        "loss_name = config.get('loss', 'mse')\n",
        "\n",
        "if not DATASETS_ROOT.exists():\n",
        "    raise FileNotFoundError(f'DATASETS_ROOT not found: {DATASETS_ROOT}')\n",
        "\n",
        "X_train, Y_train, X_test, Y_test = prepare_dataset(\n",
        "    dataset_name,\n",
        "    DATASETS_ROOT,\n",
        "    num_data,\n",
        "    classes,\n",
        "    dataset_seed=dataset_seed,\n",
        "    loss_type=loss_name,\n",
        ")\n",
        "print(f'Training tensors: {tuple(X_train.shape)} | Targets: {tuple(Y_train.shape)}')\n",
        "print(f'Test tensors: {tuple(X_test.shape)} | Targets: {tuple(Y_test.shape)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: mlp | parameters: 1,841,162\n",
            "Optimizer: SGD | lr=0.005 | batch size=16\n"
          ]
        }
      ],
      "source": [
        "model_name = config.get('model', 'mlp')\n",
        "net = prepare_net_dataset_specific(model_name, dataset_name)\n",
        "net.to(device)\n",
        "\n",
        "if not config.get('no_init', False):\n",
        "    initialize_net(net, scale=config.get('init_scale'), seed=config.get('init_seed'))\n",
        "\n",
        "learning_rate = float(LEARNING_RATE_OVERRIDE if LEARNING_RATE_OVERRIDE is not None else config.get('lr', 0.001))\n",
        "batch_size = int(BATCH_SIZE_OVERRIDE if BATCH_SIZE_OVERRIDE is not None else config.get('batch', 64))\n",
        "momentum = config.get('momentum', None)\n",
        "if isinstance(momentum, str) and momentum.lower() in {'none', 'null'}:\n",
        "    momentum = None\n",
        "momentum = None if momentum is None else float(momentum)\n",
        "adam_flag = bool(config.get('adam', False))\n",
        "\n",
        "optimizer = prepare_optimizer(net, lr=learning_rate, momentum=momentum, adam=adam_flag)\n",
        "loss_fn = build_loss_fn(loss_name)\n",
        "\n",
        "param_count = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "print(f'Model: {model_name} | parameters: {param_count:,}')\n",
        "print(f\"Optimizer: {'Adam' if adam_flag else 'SGD'} | lr={learning_rate} | batch size={batch_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Restored step=9744, epoch=19, recorded loss=0.31078240275382996\n"
          ]
        }
      ],
      "source": [
        "checkpoint_directory = Path(CHECKPOINT_DIR).expanduser().resolve() if CHECKPOINT_DIR else get_checkpoint_dir_for_run(resolved_run_id)\n",
        "if checkpoint_directory is None or not checkpoint_directory.exists():\n",
        "    raise FileNotFoundError(f\"No checkpoint directory found for run '{resolved_run_id}'.\")\n",
        "\n",
        "checkpoint_info = find_closest_checkpoint_wandb(TARGET_STEP, run_id=resolved_run_id, checkpoint_dir=checkpoint_directory)\n",
        "if checkpoint_info is None:\n",
        "    raise FileNotFoundError(f'No checkpoint available at or before step {TARGET_STEP}.')\n",
        "\n",
        "checkpoint_data = load_checkpoint_wandb(checkpoint_info, net, optimizer)\n",
        "restored_step = checkpoint_data['step']\n",
        "restored_epoch = checkpoint_data['epoch']\n",
        "restored_loss = checkpoint_data['loss']\n",
        "print(f\"Restored step={restored_step}, epoch={restored_epoch}, recorded loss={restored_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader: 8192 samples | 512 batches/epoch\n",
            "Test loader: 10000 samples | 625 batches/epoch\n"
          ]
        }
      ],
      "source": [
        "train_loader, train_cycle = make_data_cycle(X_train, Y_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, Y_test), batch_size=batch_size, shuffle=False)\n",
        "print(f'Train loader: {len(train_loader.dataset)} samples | {len(train_loader)} batches/epoch')\n",
        "print(f'Test loader: {len(test_loader.dataset)} samples | {len(test_loader)} batches/epoch')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "41b1b92b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8192, 3, 32, 32])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71f825a4",
      "metadata": {},
      "source": [
        "## Some small checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d0b7f38b",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = X_train.to(device)\n",
        "Y = Y_train.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "82642f85",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.3756, grad_fn=<MeanBackward0>)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = net(X).squeeze()\n",
        "loss = ((preds - Y)**2).sum(dim=1)\n",
        "loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d59d75",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56a9ece0",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: run additional optimizer steps (edit num_steps/log_every as needed)\n",
        "history = run_optimizer_steps(net, optimizer, train_cycle, loss_fn, num_steps=5, log_every=1)\n",
        "print('Recorded losses:', history)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
